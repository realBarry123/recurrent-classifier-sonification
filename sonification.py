import torch
import numpy as np
from scipy.io import wavfile
from torchvision import datasets, transforms
from model import RClassifier
from utils import *

def mix(audio):
    mix = audio.sum(axis=1)
    mix /= np.max(np.abs(mix))
    return np.int16(mix * 32767)

def get_stereo_masks(timesteps: int, channels: int):
    left_mask = torch.arange(1, 0, -1/channels).unsqueeze(0).repeat(timesteps, 1)
    right_mask = torch.ones(channels).unsqueeze(0).repeat(timesteps, 1) - left_mask
    return left_mask, right_mask

def get_diff_mask(history: torch.Tensor, note_length, fs=44100):
    T, C = history.shape
    samples_per_note = int(fs * note_length)
    diff = torch.cat((torch.zeros(1, C), torch.diff(history, dim=0), torch.zeros(1, C)), dim=0)
    diff = torch.abs(diff)
    diff = diff / torch.max(diff)
    diff = interpolate(diff, samples_per_note)
    diff = diff[samples_per_note // 2 : -samples_per_note // 2]
    print(diff.shape)
    print(history.shape)
    return diff

def sonify(history: torch.Tensor, note_length, fs=44100, do_stereo=True, do_interpolate=False, do_diff=False):
    """
    Partially generated by GPT-4o, GPT-5 mini (OpenAI, Jan 2026)
    """
    history = history.cpu().numpy() # (T, V)
    T, V = history.shape
    samples_per_note = int(fs * note_length)

    if do_interpolate:
        freq_samples = interpolate(history, samples_per_note).cpu().numpy()
    else:
        freq_samples = np.repeat(history, samples_per_note, axis=0)
    freq_samples = freq_samples.astype(np.float64) #?

    # Previous STAR: phase = 2 * np.pi * np.cumsum(freq_samples, axis=0) / fs
    # Ours: 
    delta = 2 * np.pi * freq_samples / float(fs) # phase increment per sample
    phase = np.cumsum(delta, axis=0) # integrate
    phase = np.mod(phase, 2 * np.pi) # wrap phase

    audio = np.sin(phase)
    if do_diff:
        diff_mask = get_diff_mask(torch.tensor(history), note_length)
        histogram(diff_mask)
        #diff_mask = torch.sigmoid(diff_mask)
        histogram(diff_mask)
        audio *= diff_mask.numpy()

    if do_stereo:
        T, C = audio.shape

        left_mask, right_mask = get_stereo_masks(T, C)

        # TODO: here arrays go torch -> numpy -> torch
        stereo = [mix(audio * left_mask.numpy()), mix(audio * right_mask.numpy())]
        stereo = torch.tensor(stereo).permute(1, 0)

        return stereo.numpy()
    else:
        wav = mix(audio)
        return wav

if __name__ == "__main__":

    DEVICE = "mps"
    MODEL_NAME = "02-03.1_16x33"
    SONIFICATION_NAME = "02-20.0.2_sigmoid-diff"
    DATA_INDEX = 2
    CONTROL = False

    transform = transforms.ToTensor()
    dataset = datasets.FashionMNIST(root=".", train=True, download=True, transform=transform)

    model, _ = load(f"models/{MODEL_NAME}.pt")
    if CONTROL: 
        model = RClassifier(**model.configs)
    model = model.to(DEVICE)
    model.T = 32

    x, y = dataset[DATA_INDEX]
    x = x.to(DEVICE)
    model(x)

    z_history = model.get_history(layer="z")
    z_history = z_history.squeeze(1)
    # Linear normalization
    z_history = (z_history - torch.min(z_history)) / (torch.max(z_history) - torch.min(z_history)) * 2000 + 50
    histogram(z_history)

    out_history = model.get_history(layer="out")
    out_history = out_history.squeeze(1)
    out_history = torch.nn.functional.softmax(out_history, dim=1)

    wavfile.write(f"{SONIFICATION_NAME}.wav", 44100, sonify(z_history[:, :], 1, do_stereo=True, do_diff=True))