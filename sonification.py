import torch
import numpy as np
from scipy.io import wavfile
from torchvision import datasets, transforms
from model import RClassifier
from utils import load, plot_history, interpolate

DEVICE = "mps"
MODEL_NAME = "01-16.0_10x33"
SONIFICATION_NAME = "01-24.1_cos_control"
DATA_INDEX = 2
CONTROL = False
DO_INTERPOLATE = False

transform = transforms.ToTensor()
dataset = datasets.FashionMNIST(root=".", train=True, download=True, transform=transform)

model, _ = load(f"models/{MODEL_NAME}.pt")
if CONTROL: 
    model = RClassifier(**model.configs)
model = model.to(DEVICE)
model.T = 128

x, y = dataset[DATA_INDEX]
x = x.to(DEVICE)
_, z_history, out_history = model(x)
z_history = z_history.squeeze(1)
z_history = torch.nn.functional.sigmoid(z_history * 10) * 2000 + 50
out_history = out_history.squeeze(1)
out_history = torch.nn.functional.softmax(out_history, dim=1)
T, C = z_history.shape
if DO_INTERPOLATE:
    z_history = interpolate(z_history, 8)

def mix(audio):
    mix = audio.sum(axis=1)
    mix /= np.max(np.abs(mix))
    return np.int16(mix * 32767)

def sonify(history: torch.Tensor, note_length, fs=44100, do_stereo=True):
    """
    Partially generated by ChatGPT (OpenAI, Jan 2026)
    """
    history = history.cpu().numpy() # (T, V)
    samples_per_note = int(fs * note_length)
    freq_samples = np.repeat(history, samples_per_note, axis=0)

    phase = 2 * np.pi * np.cumsum(freq_samples, axis=0) / fs

    audio = np.sin(phase)
    print(audio.shape)

    if do_stereo:
        T, C = audio.shape

        left_mask = torch.arange(1, 0, -1/C).unsqueeze(0).repeat(T, 1)
        right_mask = torch.ones(C).unsqueeze(0).repeat(T, 1) - left_mask

        stereo = [mix(audio * left_mask.numpy()), mix(audio * right_mask.numpy())]
        stereo = torch.tensor(stereo).permute(1, 0)

        print(stereo.shape)
        return stereo.numpy()
    else:
        wav = mix(audio)
        return wav

wavfile.write(f"{SONIFICATION_NAME}.wav", 44100, sonify(z_history[:, :], 0.48))