import torch
import numpy as np
from scipy.io import wavfile
from torchvision import datasets, transforms
from model import RClassifier
from utils import load, plot_history, interpolate
DEVICE = "mps"

transform = transforms.ToTensor()

dataset = datasets.FashionMNIST(root=".", train=True, download=True, transform=transform)

print(dataset)

model, _ = load("model.pt")
#model = RClassifier(**model.configs)
model = model.to(DEVICE)
model.T = 128

index = 2

x, y = dataset[index]
x = x.to(DEVICE)
_, z_history, out_history = model(x)
z_history = z_history.squeeze(1)
z_history = torch.nn.functional.sigmoid(z_history * 10) * 2000 + 50
# z_history = torch.nn.functional.sigmoid(z_history) * 440
out_history = out_history.squeeze(1)
out_history = torch.nn.functional.softmax(out_history, dim=1)
T, C = z_history.shape
#z_history = interpolate(z_history, 8)
#print(z_history.shape)

def mix(audio):
    mix = audio.sum(axis=1)
    mix /= np.max(np.abs(mix))
    return np.int16(mix * 32767)

def sonify(history: torch.Tensor, note_length, fs=44100, do_stereo=True):
    """
    Partially generated by ChatGPT (OpenAI, Jan 2026)
    """
    history = history.cpu().numpy() # (T, V)
    samples_per_note = int(fs * note_length)
    freq_samples = np.repeat(history, samples_per_note, axis=0)

    phase = 2 * np.pi * np.cumsum(freq_samples, axis=0) / fs

    audio = np.sin(phase)
    print(audio.shape)
    # TODO: this is the time for any sort of volume masking
    if do_stereo:
        T, C = audio.shape

        left_mask = torch.arange(1, 0, -1/C).unsqueeze(0).repeat(T, 1)
        right_mask = torch.ones(C).unsqueeze(0).repeat(T, 1) - left_mask

        stereo = [mix(audio * left_mask.numpy()), mix(audio * right_mask.numpy())]
        stereo = torch.tensor(stereo).permute(1, 0)

        print(stereo.shape)
        return stereo.numpy()
    else:
        wav = mix(audio)
        return wav

wavfile.write("stereo2.wav", 44100, sonify(z_history[:, :], 0.48))